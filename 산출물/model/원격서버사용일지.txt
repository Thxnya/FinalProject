220920

  -- 오전 진행사항 --


1) 가상 환경 및 워킹 디렉토리 작성
  - 이름은 모두 ANN

2) 오픈 소스 깃 복제 및 라이브러리 설치
  - TextRecognitionDataGenerator : 글자 이미지 생성
  - TRDG2DTRB : 이미지 라벨링
  - EasyOCR : 모델 학습 및 텍스트 인식
  
3) 이미지 생성 테스트
  - TextRecognitionDataGenerator
    - Train : 1000개, Val : 100개 생성
    - 한글 1음절로 진행
    
    
  -- 오후 진행사항 --


1) 생성한 이미지를 학습할 수 있는 데이터 형태로 전처리
  - TRDG2DTRB
    - 라벨링 데이터 생성(gt.txt)
  - data_processing.py
    - 라벨링 데이터 csv형태로 변환(labels.csv, utf8)
  
2) 테스트 학습 및 모델 생성
  - easyOCR
    - num_iter : 100, valInterval : 50
    - batch_size : 16
    - 기존 easyOCR의 한글 인식모델인 korean_g2모델에 미세조정 방식으로 진행
    - 모델 구조 : None-VGG-BiLSTM-CTC(korean_g2과 동일)
    - 학습에 사용하는 파일 : "/home/ubuntu/ANN/OpenSource/EasyOCR/trainer/ocr_trainer.py"
    - 세팅 파일 : "/home/ubuntu/ANN/OpenSource/EasyOCR/trainer/config_files/test.yaml"
    - 학습 완료된 모델 : "/home/ubuntu/ANN/OpenSource/EasyOCR/trainer/saved_models/"
	
---------------------------------------------------------------------------------------------
	
220921

  -- 오전 진행사항 --
  
  1) 학습데이터 이미지 생성(흑백)
    - 1어절
      - train : 7500
      - val : 2500
    - 1단어
      - train : 15000
      - val : 5000
    - 2단어
      - train : 18750
      - val : 6250
    - 1어절
      - train : 15000
      - val : 5000
    - 2어절 
      - train : 18750
      - val : 6250
  
  - 총 10만개의 한글 학습데이터 생성(train : val = 0.75 : 0.25)
  
  2) Korean_g2(None-VGG-BiLSTM-CTC) 모델에 전이학습진행
    - batch_size : 32
    - num_iter : 300000
    - valInterval : 2000
  
  
  -- 오후 진행사항 --
  
  1) 학습 진행도중 epochs 140000에서 종료
    - 학습 진행중 epochs 16000부터 정확도가 100%가 나오기 시작함
    - epochs 20000 모델로 샘플 영상을 인식시켰으나 결과가 전혀 맞지않았음
    - TextRecognitionDataGenerator로 생성한 데이터가 모델에 오히려 혼동을 주는 것으로 생각됨
    - 실제 강의 영상에서 가져온 이미지를 학습 데이터로 사용하면 미세조정이 훨씬 잘될 것으로 생각됨 

---------------------------------------------------------------------------------------------

220922

  -- 오전 진행사항 --

  1) Korean_g2 모델에 전이학습진행 (오전 11시 시작)
	- 기존에 생성했던 데이터로 진행
    - 모델 구조중 VGG를 ResNet으로 변경
	- batch_size : 32
    - num_iter : 300000
    - valInterval : 2000
  
  
  -- 오후 진행사항 --
  
  1) epochs 218000에서 학습종료 (오후 5시 30분 종료)
  
  2) 210000회 학습된 모델로 테스트
  validation data
  Current_accuracy : 100.000, Best_accuracy    : 100.000
  
  - 정답
	'회귀계수들의 유의성 → F-통계량으로 확인'
	'모형이 얼마나 설명력을 갖는가? → 결정계수로 확인'
	'모형이 데이터를 잘 적합하고 있는가? → 잔차와 종속변수의 산점도로 확인'
	'데이터가 전제하는 가정을 만족시키는가? → 선형성, 독립성, 등분산성, 비상관성, 정상성을 만족하는지 확인'

  - 테스트결과
	'죄귀게수움의 유타성' 'FH옆제랑으로 락인'
	'손콩이 곰마나 솜명루용 핏는가7 녀 꼼정계수로 탁인'
	'손형이 데이띄을 잘 적랍하고 있는가75 잔차와 종속변수의산점돼로 락인'
	'데이터가 쩐제하는 가경층 안즉시기는가이녀 선콩데 뿌점덧 퉁분산덧 비성관덧 정a덧용 안후차든어 락인'

---------------------------------------------------------------------------------------------

220924

  -- 오전 진행사항 --
  1) 기존에 생성해놓은 데이터로 일반학습진행
  	- 구조 : None-ResNet-BiLSTM-CTC
  	- Batch_Size : 32
  	- Epochs : 300000
  	- ValInterval : 2000
  
  - 정답
  '회귀계수들의 유의성 → F-통계량으로 확인'
  '모형이 얼마나 설명력을 갖는가? → 결정계수로 확인'
  '모형이 데이터를 잘 적합하고 있는가? → 잔차와 종속변수의 산점도로 확인'
  '데이터가 전제하는 가정을 만족시키는가? → 선형성, 독립성, 등분산성, 비상관성, 정상성을 만족하는지 확인'
  
  - 테스트결과
  '최귀계수들의 유의생 FC한계량으로 짝인'
  '모챙이널열마나 셈멈력음 곳는가긴간 결점계인로 척입'
  '모왕이 레이터틀 잘 적합허고 있는가간 수 잔차외 총속변수의 산점도로 책인'
  '레이터거조객허는 거종육 뒤측시키F거간  6행섬 독돼춘 한레엇섬 비삼관섬 결삼센빼 뒤폭허F지욱민'
   
  타겟 강의자료를 이용해서 학습데이터 생성하여 학습하면 정확도가 올라갈 것으로 예상됩니다.

---------------------------------------------------------------------------------------------

220927

  -- 진행사항 --
  1) pyscenedetect 파일 생성 후 test
  2) 폴더 생성 및 자료 저장
  3) ANN 가상환경 pip install 라이브러리 설치


---------------------------------------------------------------------------------------------

220930

  강의자료 이미지에서 글자를 추출하여 라벨링한 데이터를 학습데이터에 포함하여 학습진행
  1) 전이학습
    - 구조 : None-ResNet-BiLSTM-CTC
	- Batch_Size : 32
  	- Epochs : 20000
  	- ValInterval : 500
    
  2) 일반학습
	- 구조 : None-ResNet-BiLSTM-CTC
	- Batch_Size : 32
  	- Epochs : 20000
  	- ValInterval : 500
	
	Epochs가 2만회 근처에서 validation_accuracy가 100%이 나오기 시작하기 때문에
	그 이상 학습을 실행하는 것은 의미가 크지않다고 판단하여 Epochs를 30만회에서 2만회로 줄였습니다.
	
  - 테스트결과
  1) 전이학습
	'외귀계수청의 무의선 Fs계탕으로 쾌인'
	'모염이 털마나 설명력을 갖는가이 녀 결점계수로 확인'
	'모렁이 데이터을 잘 적립하고 있는가이 넉 잔차와 종속변수의 산점도로 확인'
	'데이터가 컨제하는 가정콤 만족시키는가이 키 선형전 옥음성 등분산성 비석관성 정석성름 만족하는지 확인'
  
  
  2) 일반학습
	'박리계수도의 유의성 F중부량으로 백인 '
	'또행이 청머나 장력을 씻는가 씨 종정계수로 확인'
	'모형이 데이터큼 잘 적합하고 있는가  A 잔차와 종속변수의산점도로 확인'
	'데이터가 컨제에는 가령을 트쪽시키도가 씨 진형실 특명성 항분의장 비짓관장 청요성용 트족하는지 확으'

---------------------------------------------------------------------------------------------

221011
  
  1) 220921에 생성했던 모델 중 가장 마지막에 생성된 모델이 아닌 
     best_accuracy 모델로 테스트를 진행했을 때, 한글 인식 정확도가 높게 나오는 점을 확인했기 때문에,
     기존에 생성한 데이터로 korean_g2 모델에 전이학습 진행
    - 구조 : None-VGG-BiLSTM-CTC
	- Batch_Size : 32
  	- Epochs : 50000
  	- ValInterval : 10000
	
  - 테스트결과
    '회귀계수들의 유의성 F-통계랑으로 확인'
	'모형이 얼아나 실명력을 강는가7 4 결정계수로 확인'
	'모형이 데이터틀 잘 적합하고 있는가7 " 잔차와 종속변수의 산점도로 확인'
	'데이터가 전제하는 가정을 만죽시키는가7 " 선형성 독립성 등분산성. 비상관 성 정상성을 만죽하는지 확인'

  한글인식의 정확도를 높이기 위해서 전이학습을 한 모델이지만
  korean_g2 모델에서는 정답이었던 글자를 틀리거나 특수문자의 경우는 정확도가 눈에 띄게 떨어졌습니다.
  다른 테스트 데이터의 경우에 영문 알파벳의 정확도가 떨어지는 경우도 있었습니다.
  아마도 학습데이터의 구성에 문제가 있었던 것으로 추측됩니다.
  진행하고 있는 프로젝트에는 기한이 한정되어있어 시도해보지 못했지만
  학습데이터를 생성할때, 영문과 특수문자를 추가하고 학습데이터에 배경 이미지를 지정해주거나 다양한 변형을 준다면
  정확도를 올릴 수 있을 것으로 생각됩니다.
  